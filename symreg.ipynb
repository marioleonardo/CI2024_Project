{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions â€” see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from icecream import ic\n",
    "\n",
    "def true_f(x: np.ndarray) -> np.ndarray:\n",
    "    return x[0] + np.sin(x[1]) / 5\n",
    "\n",
    "TEST_SIZE = 10_000\n",
    "TRAIN_SIZE = 1000\n",
    "\n",
    "x_validation = np.vstack(\n",
    "    [\n",
    "        np.random.random_sample(size=TEST_SIZE) * 2 * np.pi - np.pi,\n",
    "        np.random.random_sample(size=TEST_SIZE) * 2 - 1,\n",
    "    ]\n",
    ")\n",
    "y_validation = true_f(x_validation)\n",
    "train_indexes = np.random.choice(TEST_SIZE, size=TRAIN_SIZE, replace=False)\n",
    "x_train = x_validation[:, train_indexes]\n",
    "y_train = y_validation[train_indexes]\n",
    "assert np.all(y_train == true_f(x_train)), \"D'ho\"\n",
    "\n",
    "np.savez('problem_0.npz', x=x_train, y=y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# model 1\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value, left=None, right=None):\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def copy(self):\n",
    "        return Node(self.value, self.left.copy() if self.left else None, self.right.copy() if self.right else None)\n",
    "\n",
    "    def get_subtrees(self):\n",
    "        subtrees = []\n",
    "        if self.left:\n",
    "            subtrees.append(self.left)\n",
    "            subtrees.extend(self.left.get_subtrees())\n",
    "        if self.right:\n",
    "            subtrees.append(self.right)\n",
    "            subtrees.extend(self.right.get_subtrees())\n",
    "        return subtrees\n",
    "\n",
    "    def swap(self, other):\n",
    "        self.value, other.value = other.value, self.value\n",
    "        self.left, other.left = other.left, self.left\n",
    "        self.right, other.right = other.right, self.right\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.left and self.right:\n",
    "            return f\"({self.left} {self.value} {self.right})\"\n",
    "        return self.value\n",
    "\n",
    "class GeneticProgramming:\n",
    "    def __init__(self, population_size: int = 100, generations: int = 50):\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.best_program = None\n",
    "        self.operators = ['+', '-', '*', '/']\n",
    "        \n",
    "    def _generate_random_program(self) -> Node:\n",
    "        if random.random() < 0.5:\n",
    "            return Node(random.choice(['x[0]', 'x[1]']))\n",
    "        \n",
    "        op = random.choice(self.operators)\n",
    "        left = self._generate_random_program()\n",
    "        right = self._generate_random_program()\n",
    "        return Node(op, left, right)\n",
    "\n",
    "    def _evaluate_fitness(self, program: Node, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        try:\n",
    "            func = self.get_function_from_tree(program)\n",
    "            predictions = func(X)\n",
    "            mse = np.mean(np.square(predictions - y))\n",
    "            return 1 / (1 + mse)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def _crossover(self, parent1: Node, parent2: Node) -> Node:\n",
    "        new_parent1 = parent1.copy()\n",
    "        new_parent2 = parent2.copy()\n",
    "        \n",
    "        nodes1 = new_parent1.get_subtrees()\n",
    "        nodes2 = new_parent2.get_subtrees()\n",
    "        \n",
    "        if len(nodes1) > 0 and len(nodes2) > 0:\n",
    "            node1 = random.choice(nodes1)\n",
    "            node2 = random.choice(nodes2)\n",
    "            node1.swap(node2)\n",
    "            \n",
    "        return new_parent1\n",
    "\n",
    "    def _mutate(self, program: Node) -> Node:\n",
    "        if random.random() < 0.3:\n",
    "            nodes = program.get_subtrees()\n",
    "            if nodes:\n",
    "                node = random.choice(nodes)\n",
    "                if random.random() < 0.5:\n",
    "                    node.value = random.choice(self.operators)\n",
    "                else:\n",
    "                    node.value = random.choice(['x[0]', 'x[1]'])\n",
    "        return program\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        population = [self._generate_random_program() for _ in range(self.population_size)]\n",
    "        \n",
    "        for generation in range(self.generations):\n",
    "            fitness_scores = [(p, self._evaluate_fitness(p, X, y)) for p in population]\n",
    "            fitness_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            if self.best_program is None or fitness_scores[0][1] > self._evaluate_fitness(self.best_program, X, y):\n",
    "                self.best_program = fitness_scores[0][0].copy()\n",
    "            \n",
    "            new_population = []\n",
    "            elite_size = self.population_size // 10\n",
    "            \n",
    "            new_population.extend([p[0].copy() for p in fitness_scores[:elite_size]])\n",
    "            \n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1 = random.choice(fitness_scores[:self.population_size//2])[0]\n",
    "                parent2 = random.choice(fitness_scores[:self.population_size//2])[0]\n",
    "                child = self._crossover(parent1, parent2)\n",
    "                child = self._mutate(child)\n",
    "                new_population.append(child)\n",
    "            \n",
    "            population = new_population\n",
    "\n",
    "    def get_math_formula(self) -> str:\n",
    "        if self.best_program is None:\n",
    "            return \"No program trained yet\"\n",
    "        return str(self.best_program)\n",
    "\n",
    "    def get_function_from_tree(self, node: Node):\n",
    "        formula = str(node)\n",
    "        return lambda x: eval(formula)\n",
    "\n",
    "    def get_function_math_formula(self):\n",
    "        if self.best_program is None:\n",
    "            raise ValueError(\"No program trained yet\")\n",
    "        return self.get_function_from_tree(self.best_program)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mario\\AppData\\Local\\Temp\\ipykernel_11408\\2385175860.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return self.value(self.left.evaluate(X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Generation 1, Best Fitness: inf, Best Program Depth: 100\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Generation 2, Best Fitness: inf, Best Program Depth: 100\n",
      "Invalid prediction during evaluation\n",
      "Generation 3, Best Fitness: inf, Best Program Depth: 100\n",
      "Invalid prediction during evaluation\n",
      "Generation 4, Best Fitness: inf, Best Program Depth: 100\n",
      "Invalid prediction during evaluation\n",
      "Generation 5, Best Fitness: inf, Best Program Depth: 100\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Invalid prediction during evaluation\n",
      "Generation 6, Best Fitness: inf, Best Program Depth: 100\n",
      "Invalid prediction during evaluation\n",
      "Generation 7, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 8, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 9, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 10, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 11, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 12, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 13, Best Fitness: inf, Best Program Depth: 100\n",
      "Generation 14, Best Fitness: inf, Best Program Depth: 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 491>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    496\u001b[0m x_train, x_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split(x, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# sym1.fit(x_train, y_train)\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# y_pred = sym1.predict(x_valid)\u001b[39;00m\n\u001b[0;32m    505\u001b[0m best_function \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mget_function_math_formula()\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mSymbolicRegressor.train\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m    421\u001b[0m     offspring1, offspring2 \u001b[38;5;241m=\u001b[39m parent1\u001b[38;5;241m.\u001b[39mcopy(), parent2\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutation_rate:\n\u001b[1;32m--> 424\u001b[0m     offspring1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffspring1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutation_rate:\n\u001b[0;32m    426\u001b[0m     offspring2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation(offspring2, X_train, y_train)\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mSymbolicRegressor._mutation\u001b[1;34m(self, program, X_train, y_train)\u001b[0m\n\u001b[0;32m    279\u001b[0m         mutation_point\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# constant\u001b[39;00m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;66;03m# Optimize the constant value\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m         mutation_point\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_constant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmutated_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mutation_point\u001b[38;5;241m.\u001b[39marity \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Mutate a function\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     mutation_point\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunctions)\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mSymbolicRegressor._optimize_constant\u001b[1;34m(self, program, constant_node, X_train, y_train)\u001b[0m\n\u001b[0;32m    319\u001b[0m x0 \u001b[38;5;241m=\u001b[39m [constant_node\u001b[38;5;241m.\u001b[39mvalue]\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize\n\u001b[1;32m--> 321\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m optimized_value \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# print(f\"Optimized constant: {constant_node.value} -> {optimized_value}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    703\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 705\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1419\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1417\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m-> 1419\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m   1423\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:177\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m approx_derivative(fun_wrapped, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf,\n\u001b[0;32m    174\u001b[0m                                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinite_diff_options)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad_impl \u001b[38;5;241m=\u001b[39m update_grad\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Hessian Evaluation\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m approx_derivative(fun_wrapped, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf,\n\u001b[0;32m    174\u001b[0m                            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinite_diff_options)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[0;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[1;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(fun(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mSymbolicRegressor._optimize_constant.<locals>.objective\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m    314\u001b[0m constant_node\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m c[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    315\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m program\u001b[38;5;241m.\u001b[39mevaluate(X_train)\n\u001b[1;32m--> 316\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mSymbolicRegressor._calculate_fitness\u001b[1;34m(self, program, X, y)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;66;03m# print(\"y_pred\", y_pred)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;66;03m# print(\"y\", y)\u001b[39;00m\n\u001b[0;32m    223\u001b[0m             \u001b[38;5;66;03m# print(\"shapes\", len(y_pred), len(y.shape))\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m, \u001b[38;5;167;01mOverflowError\u001b[39;00m, \u001b[38;5;167;01mMemoryError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# print(\"Error during evaluation:\", type(err).__name__)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# print(\"Error during evaluation\")\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    405\u001b[0m     {\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    101\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:405\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03mChecks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    404\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:328\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unique1d\u001b[39m(ar, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    324\u001b[0m               return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, equal_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    Find the unique elements of an array, ignoring shape.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     ar \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     optional_indices \u001b[38;5;241m=\u001b[39m return_index \u001b[38;5;129;01mor\u001b[39;00m return_inverse\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optional_indices:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import operator\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node in the program tree.\n",
    "\n",
    "    Attributes:\n",
    "        value: The value of the node (operator, function, variable, or constant).\n",
    "        left: The left child node.\n",
    "        right: The right child node.\n",
    "        arity: The number of arguments the node's operation takes (0, 1, or 2).\n",
    "    \"\"\"\n",
    "    def __init__(self, value, left=None, right=None):\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        if self.value in SymbolicRegressor.operators:\n",
    "          if self.value == '**':\n",
    "            self.arity = 2\n",
    "          elif self.value in ['+', '-', '*', '/']:\n",
    "            self.arity = 2\n",
    "          else:\n",
    "            self.arity = 1\n",
    "        elif self.value in SymbolicRegressor.functions:\n",
    "          self.arity = 1\n",
    "        else:\n",
    "          self.arity = 0\n",
    "    def evaluate(self, X):\n",
    "        \"\"\"\n",
    "        Evaluates the node recursively for a given input.\n",
    "\n",
    "        Args:\n",
    "            X: Input data (numpy array).\n",
    "\n",
    "        Returns:\n",
    "            The result of the evaluation.\n",
    "        \"\"\"\n",
    "        if self.arity == 2:\n",
    "            if self.value == '+':\n",
    "                return self.left.evaluate(X) + self.right.evaluate(X)\n",
    "            elif self.value == '-':\n",
    "                return self.left.evaluate(X) - self.right.evaluate(X)\n",
    "            elif self.value == '*':\n",
    "                return self.left.evaluate(X) * self.right.evaluate(X)\n",
    "            elif self.value == '/':\n",
    "                # Avoid division by zero\n",
    "                divisor = self.right.evaluate(X)\n",
    "                if isinstance(divisor, np.ndarray):\n",
    "                    divisor = np.where(divisor == 0, 1e-6, divisor)  # Replace zeros with a small value\n",
    "                elif divisor == 0:\n",
    "                    divisor = 1e-6\n",
    "                return self.left.evaluate(X) / divisor\n",
    "            # elif self.value == '**':\n",
    "            #     try:\n",
    "            #         return np.power(self.left.evaluate(X), self.right.evaluate(X))\n",
    "            #     except:\n",
    "            #         print(\"left and right\", self.left.evaluate(X), self.right.evaluate(X))\n",
    "            #         return np.nan\n",
    "\n",
    "        elif self.arity == 1:\n",
    "            return self.value(self.left.evaluate(X))\n",
    "\n",
    "        elif isinstance(self.value, int):\n",
    "            if self.value < X.shape[1]:\n",
    "                return X[:, self.value]\n",
    "        else:  # constant\n",
    "            return np.array([self.value] * X.shape[0])\n",
    "\n",
    "    # Ensure that the return value is always a numpy array\n",
    "    def get_formula(self):\n",
    "        \"\"\"\n",
    "        Returns the mathematical formula represented by the node (subtree).\n",
    "        \"\"\"\n",
    "        if self.arity == 2:\n",
    "            return f\"({self.left.get_formula()} {self.value} {self.right.get_formula()})\"\n",
    "        elif self.arity == 1:\n",
    "            return f\"{self.value.__name__}({self.left.get_formula()})\"\n",
    "        elif isinstance(self.value, int):\n",
    "          return f\"x{self.value+1}\"\n",
    "        else:\n",
    "            return str(self.value)\n",
    "    \n",
    "    def get_depth(self):\n",
    "      \"\"\"Calculates the depth of the tree rooted at this node.\"\"\"\n",
    "      if self.arity == 0:\n",
    "          return 1\n",
    "      elif self.arity == 1:\n",
    "          return 1 + self.left.get_depth()\n",
    "      else:\n",
    "          return 1 + max(self.left.get_depth(), self.right.get_depth())\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Creates a deep copy of the node and its subtree.\"\"\"\n",
    "        left_copy = self.left.copy() if self.left else None\n",
    "        right_copy = self.right.copy() if self.right else None\n",
    "        return Node(self.value, left_copy, right_copy)\n",
    "    \n",
    "    def get_nodes_at_depth(self, target_depth, current_depth=0, nodes=None):\n",
    "      \"\"\"\n",
    "      Collects all nodes at a specific depth in the tree.\n",
    "\n",
    "      Args:\n",
    "          target_depth: The desired depth.\n",
    "          current_depth: The current depth during recursion.\n",
    "          nodes: A list to store the nodes found at the target depth.\n",
    "\n",
    "      Returns:\n",
    "          A list of nodes at the target depth.\n",
    "      \"\"\"\n",
    "      if nodes is None:\n",
    "          nodes = []\n",
    "\n",
    "      if current_depth == target_depth:\n",
    "          nodes.append(self)\n",
    "      elif current_depth < target_depth:\n",
    "          if self.left:\n",
    "              self.left.get_nodes_at_depth(target_depth, current_depth + 1, nodes)\n",
    "          if self.right:\n",
    "              self.right.get_nodes_at_depth(target_depth, current_depth + 1, nodes)\n",
    "      return nodes\n",
    "    \n",
    "    def replace_subtree(self, old_subtree, new_subtree):\n",
    "      \"\"\"\n",
    "      Replaces a subtree with another subtree within this node's subtree.\n",
    "\n",
    "      Args:\n",
    "          old_subtree: The subtree to be replaced.\n",
    "          new_subtree: The subtree to replace with.\n",
    "      \"\"\"\n",
    "      if self == old_subtree:\n",
    "          self.value = new_subtree.value\n",
    "          self.left = new_subtree.left\n",
    "          self.right = new_subtree.right\n",
    "          self.arity = new_subtree.arity\n",
    "          return\n",
    "\n",
    "      if self.left:\n",
    "          if self.left == old_subtree:\n",
    "              self.left = new_subtree\n",
    "              return\n",
    "          else:\n",
    "              self.left.replace_subtree(old_subtree, new_subtree)\n",
    "      if self.right:\n",
    "          if self.right == old_subtree:\n",
    "              self.right = new_subtree\n",
    "              return\n",
    "          else:\n",
    "              self.right.replace_subtree(old_subtree, new_subtree)\n",
    "          \n",
    "\n",
    "class SymbolicRegressor:\n",
    "    \"\"\"\n",
    "    A genetic programming-based symbolic regressor.\n",
    "\n",
    "    Attributes:\n",
    "        population_size: The size of the population.\n",
    "        generations: The number of generations.\n",
    "        operators: A list of allowed operators (+, -, *, /, **).\n",
    "        functions: A list of allowed functions (e.g., np.sin, np.cos).\n",
    "        const_range: The range for generating random constants.\n",
    "        init_depth: The initial depth range for generating trees.\n",
    "        crossover_rate: The probability of crossover.\n",
    "        mutation_rate: The probability of mutation.\n",
    "        p_terminal: The probability of selecting a terminal node during initialization.\n",
    "        metric: The evaluation metric ('mse').\n",
    "    \"\"\"\n",
    "    operators = ['+', '-', '*', '/', '**']\n",
    "    functions = [np.sin, np.cos, np.exp]\n",
    "\n",
    "    def __init__(self, population_size=100, generations=20, operators=['+', '-', '*', '/', '**'], functions=[np.sin, np.cos, np.exp], const_range=(-1.0, 1.0),\n",
    "                 init_depth=(2, 6), crossover_rate=0.7, mutation_rate=0.1, p_terminal=0.5, metric='mse'):\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.operators = operators\n",
    "        self.functions = functions\n",
    "        self.const_range = const_range\n",
    "        self.init_depth = init_depth\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.p_terminal = p_terminal\n",
    "        self.metric = metric\n",
    "        self.best_program = None\n",
    "        self.best_fitness = float('inf')\n",
    "\n",
    "    def _generate_random_tree(self, max_depth, X_train):\n",
    "        \"\"\"Generates a random program tree.\"\"\"\n",
    "        if max_depth == 0 or (max_depth > 0 and random.random() < self.p_terminal):\n",
    "            # Terminal node (variable or constant)\n",
    "            if random.random() < 0.5:\n",
    "                return Node(random.randint(0, X_train.shape[1]-1)) #variable\n",
    "            else:\n",
    "                return Node(random.uniform(*self.const_range)) #constant\n",
    "        else:\n",
    "            # Non-terminal node (operator or function)\n",
    "            if random.random() < len(self.operators) / (len(self.operators) + len(self.functions)):\n",
    "                op = random.choice(self.operators)\n",
    "                left = self._generate_random_tree(max_depth - 1, X_train)\n",
    "                if op == '**':\n",
    "                  right = Node(random.randint(0, 3)) # Exponentiation with small integers\n",
    "                else:\n",
    "                  right = self._generate_random_tree(max_depth - 1, X_train)\n",
    "                return Node(op, left, right)\n",
    "            else:\n",
    "                func = random.choice(self.functions)\n",
    "                return Node(func, self._generate_random_tree(max_depth - 1, X_train))\n",
    "\n",
    "    def _calculate_fitness(self, program, X, y):\n",
    "        \"\"\"Calculates the fitness of a program.\"\"\"\n",
    "        try:\n",
    "            y_pred = program.evaluate(X)\n",
    "            if self.metric == 'mse':\n",
    "                # Handle cases where y_pred might have invalid values\n",
    "                if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)) :\n",
    "                    print(\"Invalid prediction during evaluation\")\n",
    "                    return float('inf')\n",
    "                else:\n",
    "                    # print(\"y_pred\", y_pred)\n",
    "                    # print(\"y\", y)\n",
    "                    # print(\"shapes\", len(y_pred), len(y.shape))\n",
    "                    return mean_squared_error(y, y_pred)\n",
    "        except (ValueError, TypeError, ZeroDivisionError, OverflowError, MemoryError) as err:\n",
    "            # print(\"Error during evaluation:\", type(err).__name__)\n",
    "            # print(\"Error during evaluation\")\n",
    "            return float('inf')  # Penalize programs that raise errors\n",
    "\n",
    "    def _crossover(self, parent1, parent2):\n",
    "      \"\"\"\n",
    "      Performs crossover between two parent trees.\n",
    "\n",
    "      Args:\n",
    "          parent1: The first parent node.\n",
    "          parent2: The second parent node.\n",
    "\n",
    "      Returns:\n",
    "          Two offspring nodes.\n",
    "      \"\"\"\n",
    "      if parent1.arity == 0 or parent2.arity == 0:\n",
    "        return parent1.copy(), parent2.copy()\n",
    "\n",
    "      # 1. Choose a random crossover point in each parent\n",
    "      crossover_point1 = random.choice(parent1.get_nodes_at_depth(random.randint(1, parent1.get_depth()-1))) if parent1.get_depth() > 1 else parent1\n",
    "      crossover_point2 = random.choice(parent2.get_nodes_at_depth(random.randint(1, parent2.get_depth()-1))) if parent2.get_depth() > 1 else parent2\n",
    "\n",
    "      # 2. Create copies of the parents to avoid modifying them directly\n",
    "      offspring1 = parent1.copy()\n",
    "      offspring2 = parent2.copy()\n",
    "      \n",
    "      offspring1.replace_subtree(crossover_point1, crossover_point2.copy())\n",
    "      offspring2.replace_subtree(crossover_point2, crossover_point1.copy())\n",
    "      \n",
    "      return offspring1, offspring2\n",
    "\n",
    "    def _mutation(self, program, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Performs mutation on a program tree and optimizes constants recurrently.\n",
    "\n",
    "        Args:\n",
    "            program: The program node to mutate.\n",
    "            X_train: Input features.\n",
    "            y_train: Target values.\n",
    "\n",
    "        Returns:\n",
    "            The mutated and optimized program node.\n",
    "        \"\"\"\n",
    "        mutated_program = program.copy()\n",
    "        mutation_point = random.choice(\n",
    "            mutated_program.get_nodes_at_depth(\n",
    "                random.randint(0, mutated_program.get_depth() - 1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if mutation_point.arity == 0:\n",
    "            # Mutate a terminal node\n",
    "            if isinstance(mutation_point.value, int):  # variable\n",
    "                mutation_point.value = random.randint(0, X_train.shape[1]-1)\n",
    "            else:  # constant\n",
    "                # Optimize the constant value\n",
    "                mutation_point.value = self._optimize_constant(\n",
    "                    mutated_program, mutation_point, X_train, y_train\n",
    "                )\n",
    "        elif mutation_point.arity == 1:\n",
    "            # Mutate a function\n",
    "            mutation_point.value = random.choice(self.functions)\n",
    "            # Optimize constants in the subtree\n",
    "            # self._optimize_constants(mutation_point, X_train, y_train)\n",
    "        else:\n",
    "            # Mutate an operator\n",
    "            mutation_point.value = random.choice(self.operators)\n",
    "            if mutation_point.value == \"**\" and random.random() < 0.7:\n",
    "                mutation_point.right = Node(random.randint(0, 3))\n",
    "            # Optimize constants in the subtree\n",
    "            # self._optimize_constants(mutation_point, X_train, y_train)\n",
    "\n",
    "        return mutated_program\n",
    "\n",
    "    def _optimize_constant(self, program, constant_node, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Optimizes a constant node's value to minimize the fitness error.\n",
    "\n",
    "        Args:\n",
    "            program: The program tree containing the constant node.\n",
    "            constant_node: The constant node to optimize.\n",
    "            X_train: Input features.\n",
    "            y_train: Target values.\n",
    "\n",
    "        Returns:\n",
    "            The optimized constant value.\n",
    "        \"\"\"\n",
    "        def objective(c):\n",
    "            constant_node.value = c[0]\n",
    "            y_pred = program.evaluate(X_train)\n",
    "            error = self._calculate_fitness(program, X_train, y_train)\n",
    "            return error\n",
    "\n",
    "        x0 = [constant_node.value]\n",
    "        from scipy.optimize import minimize\n",
    "        result = minimize(objective, x0)\n",
    "        optimized_value = result.x[0]\n",
    "        # print(f\"Optimized constant: {constant_node.value} -> {optimized_value}\")\n",
    "        return optimized_value\n",
    "\n",
    "    def _optimize_constants(self, node, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Recursively optimizes all constants in the subtree starting at the given node.\n",
    "\n",
    "        Args:\n",
    "            node: The root node of the subtree.\n",
    "            X_train: Input features.\n",
    "            y_train: Target values.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        if node.arity == 0 and not isinstance(node.value, int):  # constant node\n",
    "            node.value = self._optimize_constant(\n",
    "                self.best_program, node, X_train, y_train\n",
    "            )\n",
    "        else:\n",
    "            self._optimize_constants(node.left, X_train, y_train)\n",
    "            self._optimize_constants(node.right, X_train, y_train)\n",
    "\n",
    "    def _prune(self, program, X, y):\n",
    "      \"\"\"\n",
    "      Prunes a program tree to simplify it while trying to maintain performance.\n",
    "\n",
    "      Args:\n",
    "          program: The program tree to prune.\n",
    "          X: The input features.\n",
    "          y: The target values.\n",
    "\n",
    "      Returns:\n",
    "          The pruned program tree.\n",
    "      \"\"\"\n",
    "      original_fitness = self._calculate_fitness(program, X, y)\n",
    "      best_pruned_program = program.copy()\n",
    "      best_pruned_fitness = original_fitness\n",
    "      \n",
    "      for depth in range(program.get_depth() - 1, 0, -1):\n",
    "          nodes_at_depth = program.get_nodes_at_depth(depth)\n",
    "          for node in nodes_at_depth:\n",
    "              if node.arity > 0:  # Only consider non-terminal nodes for pruning\n",
    "                  \n",
    "                  # Try replacing with a constant\n",
    "                  pruned_program_constant = program.copy()\n",
    "                  pruned_program_constant.replace_subtree(node, Node(random.uniform(*self.const_range)))\n",
    "                  pruned_fitness_constant = self._calculate_fitness(pruned_program_constant, X, y)\n",
    "                  \n",
    "                  if pruned_fitness_constant <= best_pruned_fitness:\n",
    "                      best_pruned_fitness = pruned_fitness_constant\n",
    "                      best_pruned_program = pruned_program_constant\n",
    "                  \n",
    "                  # Try replacing with a variable\n",
    "                  for var_index in range(X.shape[1]):\n",
    "                      pruned_program_variable = program.copy()\n",
    "                      pruned_program_variable.replace_subtree(node, Node(var_index))\n",
    "                      pruned_fitness_variable = self._calculate_fitness(pruned_program_variable, X, y)\n",
    "                      \n",
    "                      if pruned_fitness_variable <= best_pruned_fitness:\n",
    "                          best_pruned_fitness = pruned_fitness_variable\n",
    "                          best_pruned_program = pruned_program_variable\n",
    "\n",
    "      if best_pruned_fitness <= original_fitness * 1.1: # allow a small error\n",
    "        return best_pruned_program\n",
    "      else:\n",
    "        return program\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Trains the symbolic regressor.\"\"\"\n",
    "        # 1. Initialize the population\n",
    "        population = [self._generate_random_tree(random.randint(*self.init_depth), X_train) for _ in range(self.population_size)]\n",
    "\n",
    "        for generation in range(self.generations):\n",
    "            # 2. Evaluate fitness\n",
    "            fitness_scores = [self._calculate_fitness(program, X_train, y_train) for program in population]\n",
    "            # print(f\"Generation {generation+1}, Best Fitness: {(population[0].get_formula())}\")\n",
    "            # 3. Store best program\n",
    "            for i, fitness in enumerate(fitness_scores):\n",
    "                if fitness < self.best_fitness:\n",
    "                    self.best_fitness = fitness\n",
    "                    self.best_program = population[i].copy()\n",
    "\n",
    "            # 4. Selection (tournament selection)\n",
    "            selected_indices = []\n",
    "            for _ in range(self.population_size):\n",
    "                tournament = random.sample(range(self.population_size), 2) #tournament of size 2\n",
    "                winner = min(tournament, key=lambda i: fitness_scores[i])\n",
    "                selected_indices.append(winner)\n",
    "\n",
    "            # 5. Crossover and Mutation\n",
    "            new_population = []\n",
    "            for i in range(0, self.population_size, 2):\n",
    "                parent1 = population[selected_indices[i]]\n",
    "                parent2 = population[selected_indices[i+1]]\n",
    "\n",
    "                if random.random() < self.crossover_rate:\n",
    "                    offspring1, offspring2 = self._crossover(parent1, parent2)\n",
    "                else:\n",
    "                    offspring1, offspring2 = parent1.copy(), parent2.copy()\n",
    "\n",
    "                if random.random() < self.mutation_rate:\n",
    "                    offspring1 = self._mutation(offspring1, X_train, y_train)\n",
    "                if random.random() < self.mutation_rate:\n",
    "                    offspring2 = self._mutation(offspring2, X_train, y_train)\n",
    "\n",
    "                new_population.extend([offspring1, offspring2])\n",
    "\n",
    "            population = new_population\n",
    "            #pruning\n",
    "            if generation % 5 == 0:\n",
    "              for i in range(len(population)):\n",
    "                population[i] = self._prune(population[i], X_train, y_train)\n",
    "              \n",
    "            print(f\"Generation {generation+1}, Best Fitness: {self.best_fitness:.4f}, Best Program Depth: {100 if self.best_program is None else self.best_program.get_depth()}\")\n",
    "\n",
    "    def get_math_formula(self):\n",
    "        \"\"\"Returns the best program as a mathematical formula string.\"\"\"\n",
    "        if self.best_program is not None:\n",
    "            return self.best_program.get_formula()\n",
    "        else:\n",
    "            return \"No program trained yet.\"\n",
    "\n",
    "    def get_function_math_formula(self):\n",
    "      \"\"\"Returns the best program as a callable function.\"\"\"\n",
    "      if self.best_program is not None:\n",
    "          return lambda X: self.best_program.evaluate(X)\n",
    "      else:\n",
    "          return \"No program trained yet.\"\n",
    "\n",
    "# Example usage with custom functions and operators:\n",
    "X = np.random.rand(100, 3)  # 3 input features\n",
    "y = np.sin(X[:, 0]) + X[:, 1] ** 2 - 0.5 * X[:, 2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the symbolic regressor\n",
    "regressor = SymbolicRegressor(\n",
    "    population_size=4000,\n",
    "    generations=15,\n",
    "    operators=['+', '-', '*', '/'],\n",
    "    functions=[np.sin, np.cos, np.exp],\n",
    "    const_range=(-.0, 2.0),\n",
    "    init_depth=(2, 5),\n",
    "    crossover_rate=0.7,\n",
    "    mutation_rate=0.4,\n",
    "    p_terminal=0.6,\n",
    "    metric='mse'\n",
    ")\n",
    "\n",
    "# regressor.train(X_train, y_train)\n",
    "\n",
    "# # Get the best program as a mathematical formula\n",
    "# best_formula = regressor.get_math_formula()\n",
    "# print(f\"Best program (math formula): {best_formula}\")\n",
    "\n",
    "# # Get the best program as a callable function\n",
    "# best_function = regressor.get_function_math_formula()\n",
    "\n",
    "# # print(\"Best function:\", best_function.)\n",
    "# # Make predictions on the test set\n",
    "# y_pred = best_function(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Test MSE: {mse:.4f}\")\n",
    "\n",
    "\n",
    "for i in [0,2,7,8]:\n",
    "    problem = np.load(f'problem_{i}.npz')\n",
    "    x = problem['x'].T\n",
    "    y = problem['y']\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    # sym1.fit(x_train, y_train)\n",
    "    regressor.train(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Predict on test data\n",
    "    # y_pred = sym1.predict(x_valid)\n",
    "    best_function = regressor.get_function_math_formula()\n",
    "\n",
    "    # print(\"Best function:\", best_function.)\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_function(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Test MSE: {mse:.4f}\")\n",
    "\n",
    "    # Print the resulting formula\n",
    "    # print(\"Best formula:\", regressor.get_math_formula())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    11.70      2.96007e+13       22       2.9244e+13      3.28377e+13      1.81m\n",
      "   1    11.14      2.96092e+13       31      2.92748e+13      3.25603e+13      1.44m\n",
      "   2    10.92      2.96045e+13        7       2.9251e+13      3.27743e+13     50.83s\n",
      "   3    10.50      2.96021e+13        1      2.92706e+13      3.25975e+13     56.13s\n",
      "   4     9.33      2.96008e+13       13      2.91424e+13      3.37514e+13     57.55s\n",
      "   5     9.04      2.36803e+16        2      2.92603e+13      3.26902e+13     51.47s\n",
      "   6     8.62      2.96056e+13        8      2.92238e+13       3.3019e+13     49.83s\n",
      "   7     8.44      2.96054e+13        5      2.92198e+13      3.30545e+13     47.32s\n",
      "   8     8.38      2.96082e+13        5      2.92696e+13      3.26068e+13     52.44s\n",
      "   9     8.67      2.96039e+13        7      2.92366e+13      3.29039e+13     38.90s\n",
      "  10     8.68      2.96031e+13        2      2.91896e+13      3.33269e+13     27.86s\n",
      "  11     8.36      2.95994e+13        6      2.90909e+13      3.42155e+13     23.26s\n",
      "  12     8.80      2.96043e+13        5      2.91599e+13      3.35938e+13     19.57s\n",
      "  13     9.12      2.96013e+13        8      2.92229e+13      3.30273e+13     22.58s\n",
      "  14     8.21      2.96037e+13       13      2.91972e+13      3.32581e+13     17.05s\n",
      "  15     8.26      2.96051e+13       13      2.91961e+13      3.32683e+13     18.87s\n",
      "  16     8.08      2.96085e+13       13      2.92038e+13      3.31987e+13      8.53s\n",
      "  17     8.15      2.96003e+13       10      2.92771e+13      3.25389e+13      9.37s\n",
      "  18     8.69      2.95969e+13       12      2.92204e+13      3.30483e+13      4.41s\n",
      "  19     8.00      2.96045e+13        8      2.92322e+13      3.29431e+13      0.00s\n",
      "Best formula: add(sin(tan(sub(X1, X2))), abs(X1))\n",
      "Mean Squared Error on Test Set: 29671680434189.152\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    12.45      2.83441e+06        3          605.017          1327.71      2.07m\n",
      "   1    10.42          1329.11        3          595.076          1865.94      1.25m\n",
      "   2    10.72          1904.22       11          579.247          1071.93      1.08m\n",
      "   3    11.51          5536.75       16          510.852          1743.82      1.00m\n",
      "   4    11.43          46019.1       12          434.102          1185.64      1.21m\n",
      "   5    12.15          41769.2       28           346.52          454.832      1.43m\n",
      "   6    17.50           166381       18          299.365           757.16     52.19s\n",
      "   7    22.66          7619.89       18          291.413          862.944      1.07m\n",
      "   8    25.70            12016       25          280.245          942.803     51.78s\n",
      "   9    27.28          30922.6       28          272.855          1005.58     53.22s\n",
      "  10    29.94      1.27144e+06       21          267.709          1052.56      1.10m\n",
      "  11    33.77          17813.3       74          249.792          1188.85     45.95s\n",
      "  12    41.14      1.10809e+06       38          262.685          1119.73     33.91s\n",
      "  13    47.61          45810.3       60          269.608          1007.75     32.77s\n",
      "  14    57.84          64033.9       80          245.946          1233.54     35.72s\n",
      "  15    60.76          7803.94       94          252.755          1158.99     27.59s\n",
      "  16    66.85            48896       28          261.927          1076.44     22.21s\n",
      "  17    73.02           393801       46          274.618          990.534     13.19s\n",
      "  18    76.75          6120.02       46          265.378          1073.69      6.72s\n",
      "  19    75.74          84061.6       90          265.141          698.728      0.00s\n",
      "Best formula: max(X1, add(div(X1, X1), mul(mul(X0, X1), add(sub(min(sqrt(X0), add(add(div(X1, X1), mul(sqrt(X0), add(sub(tan(inv(0.106)), min(mul(mul(X0, X1), inv(0.106)), mul(sub(X0, X1), mul(mul(X0, X1), inv(0.106))))), abs(abs(min(X0, sub(tan(inv(0.106)), min(mul(max(inv(0.106), inv(0.106)), div(X1, X1)), tan(-0.901))))))))), 0.941)), min(inv(0.106), div(sin(inv(0.106)), sub(cos(-0.001), abs(mul(mul(X0, X1), inv(0.106))))))), abs(mul(mul(X0, X1), inv(0.106)))))))\n",
      "Mean Squared Error on Test Set: 385.5975702731373\n",
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    10.38      1.14614e+17       19      2.21542e+07      2.22923e+07      3.92m\n",
      "   1     8.87      5.36939e+08       19      2.19948e+07      2.30664e+07      3.53m\n",
      "   2    10.84      3.28438e+07       20      2.19385e+07       2.3789e+07      4.00m\n",
      "   3    17.77      7.18927e+08       15      2.04738e+07      2.15916e+07      6.73m\n",
      "   4    23.81      1.70106e+09       47      1.81415e+07      1.77331e+07      8.11m\n",
      "   5    27.73      7.63704e+09       53      1.63183e+07      1.55892e+07      6.37m\n",
      "   6    32.31      3.89696e+09       51      1.51953e+07      1.63368e+07      6.64m\n",
      "   7    37.70      3.14594e+09       18      1.50778e+07        1.422e+07      6.01m\n",
      "   8    51.57      4.95983e+09       54      1.44362e+07      1.55176e+07      7.58m\n",
      "   9    57.16      7.81003e+09       85      1.42423e+07      1.48367e+07      7.26m\n",
      "  10    60.10      3.96114e+10       99      1.37191e+07      1.43255e+07      5.07m\n",
      "  11    64.34      2.31467e+10       52      1.34196e+07      1.27368e+07      4.84m\n",
      "  12    71.31      1.89541e+10       51      1.25206e+07      1.23594e+07      4.62m\n",
      "  13    84.57      1.16465e+10       68      1.19238e+07       1.1525e+07      5.11m\n",
      "  14    78.01      3.03636e+10       56      9.75761e+06      1.01191e+07      3.78m\n",
      "  15    69.94      1.83094e+10       70      7.37632e+06      7.15823e+06      4.13m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m x_train, x_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split(x, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# sym1.fit(x_train, y_train)\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[43msym2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# y_pred = sym1.predict(x_valid)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m sym2\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gplearn\\genetic.py:476\u001b[0m, in \u001b[0;36mBaseSymbolic.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    472\u001b[0m n_jobs, n_programs, starts \u001b[38;5;241m=\u001b[39m _partition_estimators(\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    474\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size)\n\u001b[1;32m--> 476\u001b[0m population \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_evolve\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_programs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m                              \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# Reduce, maintaining order across different n_jobs\u001b[39;00m\n\u001b[0;32m    488\u001b[0m population \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(population))\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gplearn\\genetic.py:150\u001b[0m, in \u001b[0;36m_parallel_evolve\u001b[1;34m(n_programs, parents, X, y, sample_weight, seeds, params)\u001b[0m\n\u001b[0;32m    147\u001b[0m     program\u001b[38;5;241m.\u001b[39mraw_fitness_ \u001b[38;5;241m=\u001b[39m program\u001b[38;5;241m.\u001b[39mraw_fitness(X, y, curr_sample_weight)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_samples \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;66;03m# Calculate OOB fitness\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m         program\u001b[38;5;241m.\u001b[39moob_fitness_ \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moob_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     programs\u001b[38;5;241m.\u001b[39mappend(program)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m programs\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gplearn\\_program.py:462\u001b[0m, in \u001b[0;36m_Program.raw_fitness\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_fitness\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight):\n\u001b[0;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the raw fitness of the program according to X, y.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m \n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer:\n\u001b[0;32m    464\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gplearn\\_program.py:380\u001b[0m, in \u001b[0;36m_Program.execute\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    376\u001b[0m function \u001b[38;5;241m=\u001b[39m apply_stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    377\u001b[0m terminals \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrepeat(t, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m    378\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m X[:, t] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    379\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m apply_stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:]]\n\u001b[1;32m--> 380\u001b[0m intermediate_result \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mterminals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(apply_stack) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    382\u001b[0m     apply_stack\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gplearn\\functions.py:46\u001b[0m, in \u001b[0;36m_Function.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gplearn\\functions.py:138\u001b[0m, in \u001b[0;36m_protected_log\u001b[1;34m(x1)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Closure of log for zero and negative arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mabs(x1) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.001\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0.\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Example dataset\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(-10, 10, 100).reshape(-1, 1)  # Input feature\n",
    "y = 3 * x[:, 0]**2 - 2 * x[:, 0] + 5 + np.random.normal(0, 10, 100)  # Target variable\n",
    "problem = np.load('problem_0.npz')\n",
    "x = problem['x'].T\n",
    "y = problem['y']\n",
    "\n",
    "x.shape, y.shape\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the symbolic regressor\n",
    "sym1 = SymbolicRegressor(\n",
    "    population_size=2000,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.01,\n",
    "    p_crossover=0.7,\n",
    "    p_subtree_mutation=0.1,\n",
    "    p_hoist_mutation=0.05,\n",
    "    p_point_mutation=0.1,\n",
    "    max_samples=0.9,\n",
    "    verbose=1,\n",
    "    parsimony_coefficient=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "sym2 = SymbolicRegressor(\n",
    "    population_size=2000,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.01,\n",
    "    p_crossover=0.7,\n",
    "    p_subtree_mutation=0.1,\n",
    "    p_hoist_mutation=0.05,\n",
    "    p_point_mutation=0.1,\n",
    "    max_samples=0.9,\n",
    "    verbose=1,\n",
    "    parsimony_coefficient=0.01,\n",
    "    random_state=42,\n",
    "    function_set=['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv', 'max', 'min', 'sin', 'cos', 'tan']\n",
    ")\n",
    "\n",
    "sym3 = SymbolicRegressor(\n",
    "    population_size=2000,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.01,\n",
    "    p_crossover=0.7,\n",
    "    p_subtree_mutation=0.1,\n",
    "    p_hoist_mutation=0.05,\n",
    "    p_point_mutation=0.1,\n",
    "    max_samples=0.9,\n",
    "    verbose=1,\n",
    "    metric='mse',\n",
    "    parsimony_coefficient=0.01,\n",
    "    random_state=42,\n",
    "    function_set=['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv', 'max', 'min', 'sin', 'cos', 'tan']\n",
    ")\n",
    "\n",
    "\n",
    "#2 3 4 7 8\n",
    "s1=[23205355024424.07, 434.6308280840388, 22.667369679439936, 604.9917833023129, 11245206.75804221]\n",
    "s2=[None, 1, 0.07, 265, 3329438]\n",
    "a=[2,7,8]\n",
    "for i in [2,7,8]:\n",
    "    problem = np.load(f'problem_{i}.npz')\n",
    "    x = problem['x'].T\n",
    "    y = problem['y']\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    # sym1.fit(x_train, y_train)\n",
    "    sym3.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    # Predict on test data\n",
    "    # y_pred = sym1.predict(x_valid)\n",
    "    y_pred = sym3.predict(x_valid)\n",
    "\n",
    "    # Print the resulting formula\n",
    "    print(\"Best formula:\", sym3._program)\n",
    "\n",
    "\n",
    "    # Evaluate and visualize\n",
    "    mse = mean_squared_error(y_valid, y_pred)\n",
    "    print(f\"Mean Squared Error on Test Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train\n",
    "from sklearn.metrics import mean_squared_error\n",
    "genP = GeneticProgramming(population_size=200, generations=100)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "genP.train(x_train.T, y_train)\n",
    "\n",
    "# Get the mathematical formula as string\n",
    "print(\"Evolved formula:\", genP.get_math_formula())\n",
    "\n",
    "# Get the executable function\n",
    "evolved_function = genP.get_function_math_formula()\n",
    "\n",
    "# Evaluate the evolved function\n",
    "print(f\"MSE (train): {mean_squared_error(y_train, evolved_function(x_train))}\")\n",
    "print(f\"MSE (real) : {mean_squared_error(y_validation, evolved_function(x_validation))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
